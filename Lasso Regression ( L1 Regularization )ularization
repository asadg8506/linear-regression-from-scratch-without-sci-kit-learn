

import pandas as pd
import numpy as np

file_path = r"---------------------"
x = pd.read_csv(file_path, encoding="ISO-8859-1")

features = ['QUANTITYORDERED', 'PRICEEACH', 'MONTH_ID', 'YEAR_ID']
target = 'SALES'

x[features] = x[features].fillna(x[features].mean())

X = x[features].to_numpy()
y = x[target].to_numpy()

# Normalize features for stable gradient descent
X = (X - X.mean(axis=0)) / X.std(axis=0)
# Train-test split (80-20)
train_size = int(0.8 * len(X))
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

# Add bias term (column of ones)
X_train_bias = np.c_[np.ones((X_train.shape[0], 1)), X_train]
X_test_bias = np.c_[np.ones((X_test.shape[0], 1)), X_test]

# Initialize parameters
lambda_reg = 0.1  # L1 Regularization strength
alpha = 0.01  # Learning rate
n_iterations = 1000  # Number of iterations
m, n_features = X_train_bias.shape

theta = np.zeros(n_features)  # Initialize weights

# Gradient Descent for Lasso Regression (L1 Regularization)
for i in range(n_iterations):
    y_pred = X_train_bias @ theta  # Compute predictions
    error = y_pred - y_train  # Compute error
    
    # Compute gradient of MSE
    gradient = (2/m) * (X_train_bias.T @ error)
    
    # Apply L1 Regularization using Soft Thresholding
    theta[1:] -= alpha * (gradient[1:] + lambda_reg * np.sign(theta[1:]))

    # Bias term is updated without regularization
    theta[0] -= alpha * gradient[0]

# Make predictions on the test set
y_pred_test = X_test_bias @ theta

# Calculate Mean Squared Error (MSE)
mse = np.mean((y_test - y_pred_test) ** 2)

# Calculate R² Score
ss_total = np.sum((y_test - np.mean(y_test)) ** 2)
ss_residual = np.sum((y_test - y_pred_test) ** 2)
r2 = 1 - (ss_residual / ss_total)

print(f"Mean Squared Error: {mse}")
print(f"R² Score: {r2}")
